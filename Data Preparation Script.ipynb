{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6cffe51",
   "metadata": {},
   "source": [
    "## Web Scraping with BeautifulSoup\n",
    "\n",
    "This code snippet demonstrates web scraping using the `requests` and `BeautifulSoup` libraries in Python. The goal is to extract data from a website's HTML content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ff30fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f86b08",
   "metadata": {},
   "source": [
    "### Scraping Data From Flipkart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e7848",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. The `requests` library is imported to send HTTP requests to a web page.\n",
    "2. The `BeautifulSoup` class from the `bs4` module is imported to parse HTML content.\n",
    "3. The code starts by making an HTTP GET request to a specified URL using the `requests.get()` function.\n",
    "4. The response content is then passed to the `BeautifulSoup` constructor along with the specified parser (`html.parser` in this case). This creates a `soup` object, which represents the parsed HTML content.\n",
    "5. With the `soup` object, you can use various methods and functions provided by `BeautifulSoup` to navigate and search through the HTML structure.\n",
    "6. The code snippet demonstrates finding specific HTML elements based on their attributes using the `find_all()` method. It locates elements with the class `_2WkVRV` (brand names) and `IRpwTa` (descriptions).\n",
    "7. For each found element, it extracts the text using the `.text` attribute and stores the values in separate lists (`brand_names_list` and `descriptions_list`).\n",
    "8. It also extracts the product URLs by concatenating the base URL with the `href` attribute of each description element.\n",
    "9. Finally, the code returns the lists of brand names, descriptions, and product URLs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14714ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_data_from_flipkart(url:str):\n",
    "    header = ({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36','Accepted-Language':'en-US, en;q=0.5'})\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url,headers=header)\n",
    "    \n",
    "    # Create BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all elements with the class '_2WkVRV' (brand names)\n",
    "    brand = soup.find_all('div', {'class': '_2WkVRV'})\n",
    "    \n",
    "    # Find all elements with the class 'IRpwTa' (descriptions)\n",
    "    description = soup.find_all('a', {'class': 'IRpwTa'})\n",
    "    \n",
    "    image_url = soup.find_all('img',{'class':'_2r_T1I'})\n",
    "    \n",
    "    # Extract the text of each brand name and store it in a list\n",
    "    brand_names_list = [each.text for each in brand]\n",
    "    \n",
    "    # Extract the text of each description and store it in a list\n",
    "    descriptions_list = [each.text for each in description]\n",
    "    \n",
    "    # Extract the product URLs by concatenating the base URL with the 'href' attribute of each description element\n",
    "    product_link_list = ['https://www.flipkart.com' + each['href'] for each in description]\n",
    "    \n",
    "    image_url_list = [each['src'] for each in image_url]\n",
    "    \n",
    "    # Return the lists of brand names, descriptions, and product URLs\n",
    "    return brand_names_list, descriptions_list, product_link_list,image_url_list   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d5b81",
   "metadata": {},
   "source": [
    "## Initializing Empty Lists for Flipkart Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89edd56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flipkart_product_brand_names = []\n",
    "flipkart_product_descriptions = []\n",
    "flipkart_product_urls = []\n",
    "flipkart_product_image_urls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe0dde",
   "metadata": {},
   "source": [
    "### Extracting Men's Clothing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad27e4",
   "metadata": {},
   "source": [
    "#### Topwear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ff1ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 30/30 [00:17<00:00,  1.68page/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Total number of pages\n",
    "total_pages = 30\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.flipkart.com/clothing-and-accessories/topwear/pr?sid=clo%2Cash&otracker=categorytree&p%5B%5D=facets.ideal_for%255B%255D%3DMen&otracker=nmenu_sub_Men_0_Top+wear&page={page}'\n",
    "    \n",
    "    # Call the `scrap_data_from_flipkart()` function to get brand names, descriptions, and URLs\n",
    "    brand_names, descriptions, urls,image_urls = scrap_data_from_flipkart(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    flipkart_product_brand_names.extend(brand_names)\n",
    "    flipkart_product_descriptions.extend(descriptions)\n",
    "    flipkart_product_urls.extend(urls)\n",
    "    flipkart_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cf784",
   "metadata": {},
   "source": [
    "#### Trackpants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b3d0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 30/30 [00:29<00:00,  1.01page/s]\n"
     ]
    }
   ],
   "source": [
    "# Total number of pages\n",
    "total_pages = 30\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.flipkart.com/clothing-and-accessories/bottomwear/track-pants/men-track-pants/pr?sid=clo%2Cvua%2Cjlk%2C6ql&otracker=categorytree&otracker=nmenu_sub_Men_0_Track+pants&page={page}'\n",
    "    \n",
    "    # Call the `scrap_data_from_flipkart()` function to get brand names, descriptions, and URLs\n",
    "    brand_names, descriptions, urls,image_urls = scrap_data_from_flipkart(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    flipkart_product_brand_names.extend(brand_names)\n",
    "    flipkart_product_descriptions.extend(descriptions)\n",
    "    flipkart_product_urls.extend(urls)\n",
    "    flipkart_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60180bb2",
   "metadata": {},
   "source": [
    "#### Jeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92236a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 30/30 [00:25<00:00,  1.17page/s]\n"
     ]
    }
   ],
   "source": [
    "# Total number of pages\n",
    "total_pages = 30\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.flipkart.com/clothing-and-accessories/bottomwear/jeans/men-jeans/pr?sid=clo%2Cvua%2Ck58%2Ci51&otracker=categorytree&otracker=nmenu_sub_Men_0_Jeans&page={page}'\n",
    "    \n",
    "    # Call the `scrap_data_from_flipkart()` function to get brand names, descriptions, and URLs\n",
    "    brand_names, descriptions, urls, image_urls = scrap_data_from_flipkart(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    flipkart_product_brand_names.extend(brand_names)\n",
    "    flipkart_product_descriptions.extend(descriptions)\n",
    "    flipkart_product_urls.extend(urls)\n",
    "    flipkart_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5b002",
   "metadata": {},
   "source": [
    "#### Shorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19310048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 30/30 [00:26<00:00,  1.15page/s]\n"
     ]
    }
   ],
   "source": [
    "# Total number of pages\n",
    "total_pages = 30\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.flipkart.com/clothing-and-accessories/bottomwear/shorts/men-shorts/pr?sid=clo%2Cvua%2Ce8g%2Ckc7&otracker=categorytree&otracker=nmenu_sub_Men_0_Shorts&page={page}'\n",
    "    \n",
    "    # Call the `scrap_data_from_flipkart()` function to get brand names, descriptions, and URLs\n",
    "    brand_names, descriptions, urls, image_urls = scrap_data_from_flipkart(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    flipkart_product_brand_names.extend(brand_names)\n",
    "    flipkart_product_descriptions.extend(descriptions)\n",
    "    flipkart_product_urls.extend(urls)\n",
    "    flipkart_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b34e8c8",
   "metadata": {},
   "source": [
    "### Extracting Women's Clothing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f83c759",
   "metadata": {},
   "source": [
    "#### Ethnicwear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b91e7d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 30/30 [00:30<00:00,  1.00s/page]\n"
     ]
    }
   ],
   "source": [
    "# Total number of pages\n",
    "total_pages = 30\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.flipkart.com/clothing-and-accessories/ethnic-wear/palazzo/pr?sid=clo%2Ccfv%2Cmn6&otracker=categorytree&otracker=nmenu_sub_Women_0_Palazzos&page={page}'\n",
    "    \n",
    "    # Call the `scrap_data_from_flipkart()` function to get brand names, descriptions, and URLs\n",
    "    brand_names, descriptions, urls, image_urls = scrap_data_from_flipkart(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    flipkart_product_brand_names.extend(brand_names)\n",
    "    flipkart_product_descriptions.extend(descriptions)\n",
    "    flipkart_product_urls.extend(urls)\n",
    "    flipkart_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f99ca3",
   "metadata": {},
   "source": [
    "#### Lehnga Choli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dc152d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 30/30 [00:25<00:00,  1.17page/s]\n"
     ]
    }
   ],
   "source": [
    "# Total number of pages\n",
    "total_pages = 30\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.flipkart.com/clothing-and-accessories/lehenga-choli/women-lehenga-choli/pr?sid=clo%2Chlg%2Cwrp&otracker=categorytree&otracker=nmenu_sub_Women_0_Lehenga+Choli&page={page}'\n",
    "    \n",
    "    # Call the `scrap_data_from_flipkart()` function to get brand names, descriptions, and URLs\n",
    "    brand_names, descriptions, urls, image_urls = scrap_data_from_flipkart(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    flipkart_product_brand_names.extend(brand_names)\n",
    "    flipkart_product_descriptions.extend(descriptions)\n",
    "    flipkart_product_urls.extend(urls)\n",
    "    flipkart_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0084de75",
   "metadata": {},
   "source": [
    "#### Plazzos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0036fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 30/30 [00:28<00:00,  1.07page/s]\n"
     ]
    }
   ],
   "source": [
    "# Total number of pages\n",
    "total_pages = 30\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.flipkart.com/clothing-and-accessories/ethnic-wear/palazzo/pr?sid=clo%2Ccfv%2Cmn6&otracker=categorytree&otracker=nmenu_sub_Women_0_Palazzos&page={page}'\n",
    "    \n",
    "    # Call the `scrap_data_from_flipkart()` function to get brand names, descriptions, and URLs\n",
    "    brand_names, descriptions, urls, image_urls = scrap_data_from_flipkart(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    flipkart_product_brand_names.extend(brand_names)\n",
    "    flipkart_product_descriptions.extend(descriptions)\n",
    "    flipkart_product_urls.extend(urls)\n",
    "    flipkart_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ded47",
   "metadata": {},
   "source": [
    "#### Kurtas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cf840c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 30/30 [00:32<00:00,  1.09s/page]\n"
     ]
    }
   ],
   "source": [
    "# Total number of pages\n",
    "total_pages = 30\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.flipkart.com/clothing-and-accessories/ethnic-wear/kurtas/women-kurtas-and-kurtis/pr?sid=clo%2Ccfv%2Ccib%2Crkt&q=kurtas+kurtis&otracker=categorytree&otracker=nmenu_sub_Women_0_Kurtas+%26+Kurtis&page={page}'\n",
    "    \n",
    "    # Call the `scrap_data_from_flipkart()` function to get brand names, descriptions, and URLs\n",
    "    brand_names, descriptions, urls, image_urls = scrap_data_from_flipkart(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    flipkart_product_brand_names.extend(brand_names)\n",
    "    flipkart_product_descriptions.extend(descriptions)\n",
    "    flipkart_product_urls.extend(urls)\n",
    "    flipkart_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbc25cc",
   "metadata": {},
   "source": [
    "### Scraping Data From Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f24007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_data_from_amazon(url: str):\n",
    "    # Set the header for the request to mimic a web browser\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36',\n",
    "        'Accepted-Language': 'en-US, en;q=0.5'\n",
    "    }\n",
    "    \n",
    "    # Send a GET request to the provided URL with the specified header\n",
    "    response = requests.get(url, headers=header)\n",
    "    \n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all elements with class 'a-size-base-plus a-color-base' to extract brand names\n",
    "    brand = soup.find_all('span', {'class': 'a-size-base-plus a-color-base'})\n",
    "    \n",
    "    # Find all elements with class 'a-size-base-plus a-color-base a-text-normal' to extract descriptions\n",
    "    description = soup.find_all('span', {'class': 'a-size-base-plus a-color-base a-text-normal'})\n",
    "    \n",
    "    # Find all <a> tags with class 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal' to extract URLs\n",
    "    urls = soup.find_all('a', {'class': 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'})\n",
    "    \n",
    "    image_urls = soup.find_all('img',{'class':'s-image'})\n",
    "    \n",
    "    # Extract the text content from the 'brand' elements and store them in a list\n",
    "    brand_names_list = [each.text for each in brand]\n",
    "    \n",
    "    # Extract the text content from the 'description' elements and store them in a list\n",
    "    descriptions_list = [each.text for each in description]\n",
    "    \n",
    "    # Create a list of product URLs by appending the href attribute to 'https://www.amazon.in'\n",
    "    product_link_list = ['https://www.amazon.in' + each['href'] for each in urls]\n",
    "    \n",
    "    image_urls_list = [each['src'] for each in image_urls]\n",
    "    \n",
    "    # Return the lists of brand names, descriptions, and product URLs\n",
    "    return brand_names_list, descriptions_list, product_link_list,image_urls_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ccce0",
   "metadata": {},
   "source": [
    "1. Define a function `scrap_data_from_amazon(url: str)` to scrape data from Amazon.\n",
    "2. Set the header for the request to mimic a web browser. This ensures that the request appears as if it is coming from a web browser rather than a bot.\n",
    "3. Send a GET request to the provided URL with the specified header using the `requests.get()` function. Store the response in the `response` variable.\n",
    "4. Create a BeautifulSoup object to parse the HTML content of the response. Pass the `response.content` and `'html.parser'` as arguments to the `BeautifulSoup` constructor. Store the BeautifulSoup object in the `soup` variable.\n",
    "5. Use the `soup.find_all()` method to find all elements with class 'a-size-base-plus a-color-base' and extract the brand names. Store the results in the `brand` variable.\n",
    "6. Use the `soup.find_all()` method to find all elements with class 'a-size-base-plus a-color-base a-text-normal' and extract the descriptions. Store the results in the `description` variable.\n",
    "7. Use the `soup.find_all()` method to find all `<a>` tags with class 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal' and extract the URLs. Store the results in the `urls` variable.\n",
    "8. Extract the text content from the `brand` elements and store them in a list using a list comprehension. Store the list in the `brand_names_list` variable.\n",
    "9. Extract the text content from the `description` elements and store them in a list using a list comprehension. Store the list in the `descriptions_list` variable.\n",
    "10. Create a list of product URLs by appending the href attribute of each `<a>` tag in `urls` to 'https://www.amazon.in'. Store the list in the `product_link_list` variable using a list comprehension.\n",
    "11. Return the lists of brand names, descriptions, and product URLs as a tuple: `brand_names_list`, `descriptions_list`, `product_link_list`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ee74b9",
   "metadata": {},
   "source": [
    "## Initializing Empty Lists for Amazon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18b79cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_product_brand_names = []\n",
    "amazon_product_descriptions = []\n",
    "amazon_product_urls = []\n",
    "amazon_product_image_urls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc6921f",
   "metadata": {},
   "source": [
    "### Extracting Men's Clothing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e3552d",
   "metadata": {},
   "source": [
    "#### T-Shirts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0779c262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 40/40 [01:02<00:00,  1.57s/page]\n"
     ]
    }
   ],
   "source": [
    "# Set the total number of pages\n",
    "total_pages = 40\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "# Iterate over each page\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.amazon.in/s?i=apparel&rh=n%3A1968120031%2Cp_36%3A4595084031&page=2&content-id=amzn1.sym.f5e83e00-a666-492b-b882-5fa6fba3548e&pd_rd_r=21025b0c-ec69-41d2-96e6-ec699afe0fee&pd_rd_w=lDWwe&pd_rd_wg=IQSQx&pf_rd_p=f5e83e00-a666-492b-b882-5fa6fba3548e&pf_rd_r=4SSBTYVCG98DM53DR4PQ&ref=sr_pg_{page}'\n",
    "    \n",
    "    # Scrape data from the Amazon page using the provided URL\n",
    "    brand_names, descriptions, urls, image_urls = scrap_data_from_amazon(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    amazon_product_brand_names.extend(brand_names)\n",
    "    amazon_product_descriptions.extend(descriptions)\n",
    "    amazon_product_urls.extend(urls)\n",
    "    amazon_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d80071",
   "metadata": {},
   "source": [
    "#### Jeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d355c0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 40/40 [01:05<00:00,  1.63s/page]\n"
     ]
    }
   ],
   "source": [
    "# Set the total number of pages\n",
    "total_pages = 40\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "# Iterate over each page\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.amazon.in/s?i=apparel&rh=n%3A1968076031&fs=true&page=2&ref=sr_pg_{page}'\n",
    "    \n",
    "    # Scrape data from the Amazon page using the provided URL\n",
    "    brand_names, descriptions, urls, image_urls = scrap_data_from_amazon(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    amazon_product_brand_names.extend(brand_names)\n",
    "    amazon_product_descriptions.extend(descriptions)\n",
    "    amazon_product_urls.extend(urls)\n",
    "    amazon_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ef575",
   "metadata": {},
   "source": [
    "#### Jeans & Coats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c2cb403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 40/40 [00:57<00:00,  1.45s/page]\n"
     ]
    }
   ],
   "source": [
    "# Set the total number of pages\n",
    "total_pages = 40\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "# Iterate over each page\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.amazon.in/s?i=apparel&rh=n%3A1968088031&fs=true&page=2&qid=1684610738&ref=sr_pg_{page}'\n",
    "    \n",
    "    # Scrape data from the Amazon page using the provided URL\n",
    "    brand_names, descriptions, urls, image_urls = scrap_data_from_amazon(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    amazon_product_brand_names.extend(brand_names)\n",
    "    amazon_product_descriptions.extend(descriptions)\n",
    "    amazon_product_urls.extend(urls)\n",
    "    amazon_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72c7bf9",
   "metadata": {},
   "source": [
    "#### Sweaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cfebc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 40/40 [01:02<00:00,  1.57s/page]\n"
     ]
    }
   ],
   "source": [
    "# Set the total number of pages\n",
    "total_pages = 40\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "# Iterate over each page\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.amazon.in/s?i=apparel&rh=n%3A1968077031&fs=true&page=2&qid=1684610897&ref=sr_pg_{page}'\n",
    "    \n",
    "    # Scrape data from the Amazon page using the provided URL\n",
    "    brand_names, descriptions, urls, image_urls = scrap_data_from_amazon(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    amazon_product_brand_names.extend(brand_names)\n",
    "    amazon_product_descriptions.extend(descriptions)\n",
    "    amazon_product_urls.extend(urls)\n",
    "    amazon_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721281df",
   "metadata": {},
   "source": [
    "### Extracting Women's Clothing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6a5bf",
   "metadata": {},
   "source": [
    "#### Kurtas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a91b6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 40/40 [01:04<00:00,  1.62s/page]\n"
     ]
    }
   ],
   "source": [
    "# Set the total number of pages\n",
    "total_pages = 40\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "# Iterate over each page\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.amazon.in/s?i=apparel&rh=n%3A1968255031&fs=true&page=2&qid=1684611025&ref=sr_pg_{page}'\n",
    "    \n",
    "    # Scrape data from the Amazon page using the provided URL\n",
    "    brand_names, descriptions, urls, image_urls = scrap_data_from_amazon(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    amazon_product_brand_names.extend(brand_names)\n",
    "    amazon_product_descriptions.extend(descriptions)\n",
    "    amazon_product_urls.extend(urls)\n",
    "    amazon_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7991c87",
   "metadata": {},
   "source": [
    "#### Western Wear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f7f1413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 40/40 [00:58<00:00,  1.46s/page]\n"
     ]
    }
   ],
   "source": [
    "# Set the total number of pages\n",
    "total_pages = 40\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "# Iterate over each page\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.amazon.in/s?i=apparel&rh=n%3A11400137031&fs=true&page=2&ref=sr_pg_{page}'\n",
    "    \n",
    "    # Scrape data from the Amazon page using the provided URL\n",
    "    brand_names, descriptions, urls, image_urls = scrap_data_from_amazon(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    amazon_product_brand_names.extend(brand_names)\n",
    "    amazon_product_descriptions.extend(descriptions)\n",
    "    amazon_product_urls.extend(urls)\n",
    "    amazon_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3abf99",
   "metadata": {},
   "source": [
    "#### Salwar Suits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e0062c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 40/40 [01:03<00:00,  1.58s/page]\n"
     ]
    }
   ],
   "source": [
    "# Set the total number of pages\n",
    "total_pages = 40\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "# Iterate over each page\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.amazon.in/s?i=apparel&rh=n%3A3723380031&fs=true&page=2&ref=sr_pg_{page}'\n",
    "    \n",
    "    # Scrape data from the Amazon page using the provided URL\n",
    "    brand_names, descriptions, urls, image_urls = scrap_data_from_amazon(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    amazon_product_brand_names.extend(brand_names)\n",
    "    amazon_product_descriptions.extend(descriptions)\n",
    "    amazon_product_urls.extend(urls)\n",
    "    amazon_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb6e4f",
   "metadata": {},
   "source": [
    "#### Sarees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15116a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 40/40 [01:03<00:00,  1.60s/page]\n"
     ]
    }
   ],
   "source": [
    "# Set the total number of pages\n",
    "total_pages = 40\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_pages, unit='page')\n",
    "\n",
    "# Iterate over each page\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Create the URL for the specific page\n",
    "    url = f'https://www.amazon.in/s?i=apparel&rh=n%3A1968256031&fs=true&page=2&ref=sr_pg_{page}'\n",
    "    \n",
    "    # Scrape data from the Amazon page using the provided URL\n",
    "    brand_names, descriptions, urls, image_urls = scrap_data_from_amazon(url)\n",
    "    \n",
    "    # Extend the respective lists with the obtained data from the current page\n",
    "    amazon_product_brand_names.extend(brand_names)\n",
    "    amazon_product_descriptions.extend(descriptions)\n",
    "    amazon_product_urls.extend(urls)\n",
    "    amazon_product_image_urls.extend(image_urls)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd7b2b3",
   "metadata": {},
   "source": [
    "# Combining Scrapped data of Amazon & Flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6e62eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the brand names from Flipkart and Amazon\n",
    "combined_brands = flipkart_product_brand_names + amazon_product_brand_names\n",
    "\n",
    "# Combine the product descriptions from Flipkart and Amazon\n",
    "combined_product_descriptions = flipkart_product_descriptions + amazon_product_descriptions\n",
    "\n",
    "# Combine the product URLs from Flipkart and Amazon\n",
    "combined_product_urls = flipkart_product_urls + amazon_product_urls\n",
    "\n",
    "# Combine the image URLs from Flipkart and Amazon\n",
    "combined_image_urls = flipkart_product_image_urls + amazon_product_image_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621c4fb",
   "metadata": {},
   "source": [
    "# Converting These Lists into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b028f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e4be08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary named 'data' to store the data for the DataFrame\n",
    "# The keys in the dictionary represent the column names in the DataFrame\n",
    "# The values are lists containing the data for each column\n",
    "data = {'brand': combined_brands, 'description': combined_product_descriptions, 'url': combined_product_urls,'img':combined_image_urls}\n",
    "\n",
    "# Convert the 'data' dictionary to a DataFrame using the pd.DataFrame() function\n",
    "# Each key in the 'data' dictionary will become a column in the DataFrame\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4805ec",
   "metadata": {},
   "source": [
    "# Saving DataFrame as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb577da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a CSV file with the filename 'clothing_data.csv'\n",
    "# Set the parameter 'index' to False to exclude the row index labels from the CSV file\n",
    "df.to_csv('clothing_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccd3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
